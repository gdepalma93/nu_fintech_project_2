{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\GrantDePalma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initial Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from joblib import dump,load # Save Models\n",
    "from numpy import random\n",
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SDK Imports\n",
    "import tweepy\n",
    "import shrimpy\n",
    "from newsapi import NewsApiClient\n",
    "import cryptocompare\n",
    "\n",
    "# NLP Models\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "#from ibm_watson import ToneAnalyzerV3\n",
    "#from ibm_cloud_sdk.authenticators import IAMAuthenticator\n",
    "# SKLearn Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScalar\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "# Tensor Flow Models\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HV Plot\n",
    "#import hvplot.pandas\n",
    "#import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables from the .env file\n",
    "env_path = Path(\"/Users/GrantDePalma\")/'.env'\n",
    "load_dotenv(env_path)\n",
    "# Extract API Keys from environment variables\n",
    "shrimpy_public_key = os.getenv(\"SHRIMPY_PUBLIC_KEY\")\n",
    "shrimpy_private_key = os.getenv(\"SHRIMPY_PRIVATE_KEY\")\n",
    "KRAKEN_API_KEY = os.getenv(\"KRAKEN_API_KEY\")\n",
    "KRAKEN_PRIVATE_KEY = os.getenv(\"KRAKEN_PRIVATE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrimpy API Client\n",
    "shrimpy_client = shrimpy.ShrimpyApiClient(shrimpy_public_key, shrimpy_private_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nvt(symbol):\n",
    "    # Get Into The Block blockchain data from CryptoCompare API\n",
    "    url = 'https://min-api.cryptocompare.com/data/blockchain/histo/day?api_key=015b242e7bed3700ca69182009d5c7af87bf6b83f7c5a0e12eb5a9b01bee3fb0&limit=2000&fsym=' + symbol\n",
    "    data = requests.get(url).json()[\"Data\"][\"Data\"]\n",
    "    df = pd.DataFrame(data) \n",
    "    \n",
    "    # Calculate \n",
    "    df['nvt'] = df['current_supply'] / df['transaction_count'] / df['average_transaction_value']\n",
    "    \n",
    "    # Get pricing data from CryptoCompare API\n",
    "    price_url = 'https://min-api.cryptocompare.com/data/v2/histoday?tsym=USD&limit=2000&api_key=015b242e7bed3700ca69182009d5c7af87bf6b83f7c5a0e12eb5a9b01bee3fb0&fsym=' + symbol\n",
    "    histo_data = requests.get(price_url).json()[\"Data\"][\"Data\"]\n",
    "    \n",
    "    df_price = pd.DataFrame(histo_data)\n",
    "    \n",
    "    combined = df.set_index('time').join(df_price.set_index('time'), on='time')\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def get_social(symbol):\n",
    "    url = \"https://min-api.cryptocompare.com/data/social/coin/histo/day?api_key=015b242e7bed3700ca69182009d5c7af87bf6b83f7c5a0e12eb5a9b01bee3fb0&limit=2000&fsym=\" + symbol\n",
    "    data = requests.get(url).json()[('Data')]\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    price_url = 'https://min-api.cryptocompare.com/data/v2/histoday?tsym=USD&limit=2000&api_key=015b242e7bed3700ca69182009d5c7af87bf6b83f7c5a0e12eb5a9b01bee3fb0&fsym=' + symbol\n",
    "    histo_data = requests.get(price_url).json()[\"Data\"][\"Data\"]\n",
    "    \n",
    "    df_price = pd.DataFrame(histo_data)\n",
    "    \n",
    "    combined = df.set_index('time').join(df_price.set_index('time'), on='time')\n",
    "    return combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_btc_df():\n",
    "    btc_df = calc_nvt('BTC')\n",
    "    btc_df.reset_index(inplace = True)\n",
    "    btc_df['time'] = pd.to_datetime(btc_df['time'], unit = 's')\n",
    "    btc_df.drop(columns=['id', 'zero_balance_addresses_all_time', 'block_height', 'block_time', 'high', 'low', 'open', 'volumefrom', 'volumeto', 'conversionType', 'conversionSymbol'], inplace = True)\n",
    "    btc_df.set_index(\"time\", drop=True, inplace=True)\n",
    "    btc_df['returns']= btc_df['close'].pct_change() \n",
    "    btc_df['sma200'] = btc_df.close.rolling(window=200).mean()\n",
    "    btc_df['mayer_multiple'] = btc_df.close / btc_df.sma200\n",
    "    \n",
    "    quandl_mining_df = quandl.get(\"BITCOINWATCH/MINING\", api_key=q_api)\n",
    "    market_cap = pd.DataFrame(quandl_mining_df['Market Cap'])\n",
    "    market_cap.reset_index(inplace=True)\n",
    "    market_cap['time'] = market_cap['Date']\n",
    "    market_cap.drop(columns='Date',inplace=True)\n",
    "    market_cap.set_index('time', inplace=True)\n",
    "        \n",
    "    #mining_operating_margin = quandl.get(\"BCHAIN/MIOPM\", api_key=q_api)\n",
    "    #mining_operating_margin.columns = ['mining_op_margin']\n",
    "    #mining_operating_margin.reset_index(inplace=True)\n",
    "    #mining_operating_margin['time'] = mining_operating_margin['Date']\n",
    "    #mining_operating_margin.drop(columns='Date', inplace=True)\n",
    "    #mining_operating_margin.set_index('time', inplace=True)\n",
    "    #\n",
    "    fng_df = pd.DataFrame(requests.get(\"https://api.alternative.me/fng/?limit=0\").json()['data'])\n",
    "    fng_df['time'] = pd.to_datetime(fng_df['timestamp'], unit = 's')\n",
    "    fng_df.set_index(\"time\", drop=True, inplace=True)\n",
    "    fng_df.sort_index(axis=0, ascending=True, inplace=True)\n",
    "    fng_df.drop(columns=['time_until_update', 'timestamp', 'value_classification'], inplace = True)\n",
    "    fng_df.columns = ['fng_value']\n",
    "    fng_df[\"fng_value\"] = pd.to_numeric(fng_df[\"fng_value\"], downcast=\"float\")\n",
    "    fng_df['fng_value_1d']=fng_df['fng_value'].pct_change()\n",
    "    fng_df['fng_value_3d']=fng_df['fng_value'].pct_change(3)\n",
    "    fng_df['fng_value_7d']=fng_df['fng_value'].pct_change(7)\n",
    "    fng_df['fng_value_sma3_1d'] = fng_df.fng_value_1d.rolling(window=3).mean()\n",
    "    fng_df['fng_value_sma7_1d'] = fng_df.fng_value_1d.rolling(window=7).mean()\n",
    "    fng_df.dropna(inplace=True)\n",
    "    #\n",
    "    social_df = get_social('BTC')\n",
    "    social_df.reset_index(inplace = True)\n",
    "    social_df['time'] = pd.to_datetime(social_df['time'], unit = 's')\n",
    "    social_df.set_index(\"time\", drop=True, inplace=True)\n",
    "    social_df = social_df[826:-1]\n",
    "    social_df = social_df.drop(columns=['high', 'low', 'open', 'volumefrom', 'volumeto', 'conversionType', 'conversionSymbol', 'close', 'analysis_page_views', 'charts_page_views', 'code_repo_closed_issues', 'code_repo_closed_pull_issues', 'code_repo_contributors', 'code_repo_forks', 'code_repo_open_issues', 'code_repo_open_pull_issues', 'code_repo_stars', 'code_repo_subscribers', 'comments', 'fb_likes', 'fb_talking_about', 'followers', 'forum_page_views', 'influence_page_views', 'markets_page_views',\n",
    "       'overview_page_views', 'points', 'posts', 'reddit_subscribers', 'total_page_views', 'trades_page_views', 'twitter_favourites', 'twitter_following', 'twitter_lists'])\n",
    "    social_df['reddit_active_users_incr'] = social_df.reddit_active_users.pct_change()\n",
    "    social_df['reddit_comments_hr_inc'] = social_df.reddit_comments_per_hour.pct_change()\n",
    "    social_df['reddit_comments_day_inc'] = social_df.reddit_comments_per_day.pct_change()\n",
    "    social_df['reddit_posts_day_inc'] = social_df.reddit_posts_per_day.pct_change()\n",
    "    social_df['reddit_posts_hour_inc'] = social_df.reddit_posts_per_hour.pct_change()\n",
    "    social_df['twitter_followers_incr'] = social_df.twitter_followers.pct_change()\n",
    "    social_df['twitter_statuses_incr'] = social_df.twitter_statuses.pct_change()\n",
    "    #\n",
    "    rev = quandl.get(\"BCHAIN/MIREV\", api_key=q_api)\n",
    "    rev.columns = ['miners_rev']\n",
    "    rev.reset_index(inplace=True)\n",
    "    rev['time'] = rev['Date']\n",
    "    rev.drop(columns='Date', inplace=True)\n",
    "    rev.set_index('time', inplace=True)\n",
    "    #\n",
    "    btc_df0 = pd.merge(btc_df, market_cap, on=['time'], how='left')\n",
    "    btc_df1 = pd.merge(btc_df0, fng_df, on=['time'], how='left')\n",
    "    btc_df2 = pd.merge(btc_df1, rev, on='time', how='left')\n",
    "    btc_df3 = pd.merge(btc_df2, social_df, on='time', how='left')\n",
    "    \n",
    "    btc_df3['address_growth'] = btc_df3['new_addresses']/btc_df3['unique_addresses_all_time']\n",
    "    btc_df3['perc_mined'] = btc_df3['current_supply']/21000000\n",
    "    #btc_df3['price_change_1d'] = btc_df3['close'].pct_change(1)\n",
    "    #btc_df3['price_change_7d'] = btc_df3['close'].pct_change(7)\n",
    "    #btc_df3['price_change_30d'] = btc_df3['close'].pct_change(30)\n",
    "    btc_df3.drop(columns=['block_size', 'average_transaction_value', 'difficulty', 'hashrate', 'symbol', 'sma200', 'transaction_count', \n",
    "                         'transaction_count_all_time', 'unique_addresses_all_time', 'reddit_active_users', 'twitter_followers', 'twitter_statuses', 'reddit_comments_per_day', 'reddit_comments_per_hour', 'reddit_posts_per_day', 'reddit_posts_per_hour'], inplace=True)\n",
    "    btc_df3 = btc_df3.dropna()\n",
    "    return btc_df3\n",
    "\n",
    "btc_fng_df = calc_btc_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Lasso_coef_df(btc_fng_df):\n",
    "    # Set feature set and drop unnecessary columns\n",
    "    X = btc_fng_df.drop(columns=['close', 'returns'])\n",
    "    # Set target and reshape\n",
    "    y = btc_fng_df['returns'].values.reshape(-1,1)\n",
    "    \n",
    "    # Reserving 30% for test sample\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.3, random_state=31)\n",
    "    \n",
    "    #Initialize MinMax scaler function\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    #X_train scaler into a DF\n",
    "    X_train_mm_scaled = pd.DataFrame(mm_scaler.fit_transform(X_train),columns = X_train.columns, index = X_train.index)\n",
    "    # X test scaler based on X training set\n",
    "    X_mm_scaler = mm_scaler.fit(X_train)\n",
    "    X_test_mm_scaled = X_mm_scaler.transform(X_test)\n",
    "    #y_train scaler\n",
    "    y_train_mm_scaled = mm_scaler.fit_transform(y_train)\n",
    "    # y test scaler based on y training set\n",
    "    y_mm_scaler = mm_scaler.fit(y_train)\n",
    "    y_test_mm_scaled = y_mm_scaler.transform(y_test)\n",
    "    \n",
    "    lasso = Lasso(alpha=0.00005,normalize=True)\n",
    "    # Fit the regressor to the data\n",
    "    lasso.fit(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    # Compute and print the coefficients\n",
    "    lasso_coef = lasso.coef_\n",
    "    \n",
    "    columns = X_train_mm_scaled.columns\n",
    "    lasso_coef_df = pd.DataFrame(lasso_coef)\n",
    "    lasso_coef_df.rename(columns={0:'lasso_coef'}, inplace=True)\n",
    "    lasso_coef_df.index = columns\n",
    "    lasso_coef_df['lasso_coef_abs_value'] = abs(lasso_coef_df['lasso_coef'])\n",
    "    lasso_coef_df.sort_values(by=['lasso_coef_abs_value'], ascending=False, inplace=True)\n",
    "    \n",
    "    return lasso_coef_df\n",
    "\n",
    "def Lasso_viz(btc_fng_df):\n",
    "    # Set feature set and drop unnecessary columns\n",
    "    X = btc_fng_df.drop(columns=['close', 'returns'])\n",
    "    # Set target and reshape\n",
    "    y = btc_fng_df['returns'].values.reshape(-1,1)\n",
    "    # Reserving 30% for test sample\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.3, random_state=31)\n",
    "    #Initialize MinMax scaler function\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    #X_train scaler into a DF\n",
    "    X_train_mm_scaled = pd.DataFrame(mm_scaler.fit_transform(X_train),columns = X_train.columns, index = X_train.index)\n",
    "    # X test scaler based on X training set\n",
    "    X_mm_scaler = mm_scaler.fit(X_train)\n",
    "    X_test_mm_scaled = X_mm_scaler.transform(X_test)\n",
    "    #y_train scaler\n",
    "    y_train_mm_scaled = mm_scaler.fit_transform(y_train)\n",
    "    # y test scaler based on y training set\n",
    "    y_mm_scaler = mm_scaler.fit(y_train)\n",
    "    y_test_mm_scaled = y_mm_scaler.transform(y_test)\n",
    "    lasso = Lasso(alpha=0.00005,normalize=True)\n",
    "    # Fit the regressor to the data\n",
    "    lasso.fit(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    # Compute and print the coefficients\n",
    "    lasso_coef = lasso.coef_\n",
    "    \n",
    "    columns = X_train_mm_scaled.columns\n",
    "    lasso_coef_df = pd.DataFrame(lasso_coef)\n",
    "    lasso_coef_df.rename(columns={0:'lasso_coef'}, inplace=True)\n",
    "    lasso_coef_df.index = columns\n",
    "    lasso_coef_df['lasso_coef_abs_value'] = abs(lasso_coef_df['lasso_coef'])\n",
    "    lasso_coef_df.sort_values(by=['lasso_coef_abs_value'], ascending=False, inplace=True)\n",
    "    \n",
    "    print(lasso_coef)\n",
    "    # Plot the coefficients\n",
    "    plt.plot(range(len(X_train_mm_scaled.columns)), lasso_coef)\n",
    "    plt.xticks(range(len(X_train_mm_scaled.columns)), X_train_mm_scaled.columns.values, rotation=90)\n",
    "    plt.margins(0.01)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.show()\n",
    "    \n",
    "    lasso = Lasso()\n",
    "    lasso.fit(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    train_score=lasso.score(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    test_score=lasso.score(X_test_mm_scaled,y_test_mm_scaled)\n",
    "    coeff_used = np.sum(lasso.coef_!=0)\n",
    "    print (\"training score:\", train_score)\n",
    "    print (\"test score: \", test_score)\n",
    "    print (\"number of features used: \", coeff_used)\n",
    "\n",
    "    lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
    "    lasso001.fit(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    train_score001=lasso001.score(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    test_score001=lasso001.score(X_test_mm_scaled,y_test_mm_scaled)\n",
    "    coeff_used001 = np.sum(lasso001.coef_!=0)\n",
    "    print (\"training score for alpha=0.01:\", train_score001)\n",
    "    print (\"test score for alpha =0.01: \", test_score001)\n",
    "    print (\"number of features used: for alpha =0.01:\", coeff_used001)\n",
    "    lasso000001 = Lasso(alpha=0.00001, max_iter=10e5)\n",
    "    lasso000001.fit(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    train_score000001=lasso000001.score(X_train_mm_scaled,y_train_mm_scaled)\n",
    "    test_score000001=lasso000001.score(X_test_mm_scaled,y_test_mm_scaled)\n",
    "    coeff_used000001 = np.sum(lasso000001.coef_!=0)\n",
    "    print (\"training score for alpha=0.0001:\", train_score000001) \n",
    "    print (\"test score for alpha =0.0001: \", test_score000001)\n",
    "    print (\"number of features used: for alpha =0.0001:\", coeff_used000001)\n",
    "\n",
    "    #lr = LinearRegression()\n",
    "    #lr.fit(X_train_ss_scaled,y_train_ss_scaled)\n",
    "    #lr_train_score=lr.score(X_train_ss_scaled,y_train_ss_scaled)\n",
    "    #lr_test_score=lr.score(X_test_ss_scaled,y_test_ss_scaled)\n",
    "    #print (\"LR training score:\", lr_train_score) \n",
    "    #print (\"LR test score: \", lr_test_score)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "    plt.plot(lasso000001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.0001$') # alpha here is for transparency\n",
    "\n",
    "    plt.xlabel('Coefficient Index',fontsize=16)\n",
    "    plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "    plt.legend(fontsize=10,loc=4)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "    plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "    plt.plot(lasso000001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; $\\alpha = 0.00001$') # alpha here is for transparency\n",
    "    #plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)\n",
    "    plt.xlabel('Coefficient Index',fontsize=16)\n",
    "    plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "    plt.legend(fontsize=9, loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alt_df():\n",
    "    # Fetch Assets Available on Kraken\n",
    "    def get_kraken_assets():\n",
    "        kraken_assets = shrimpy_client.get_exchange_assets('kraken')\n",
    "        kraken_assets_df = pd.DataFrame(columns=['id','name', 'symbol', 'trading_symbol'])\n",
    "        for key, value in enumerate(kraken_assets):\n",
    "            kraken_assets_df.loc[key] = [value['id'], value['name'], value['symbol'], value['tradingSymbol']]\n",
    "        return kraken_assets_df\n",
    "    kraken_assets_df = get_kraken_assets()\n",
    "    \n",
    "        # Fetch Trading Pairs on Kraken\n",
    "    def calc_kraken_pairs_dataframe():\n",
    "        kraken_pairs = shrimpy_client.get_trading_pairs('kraken')\n",
    "        kraken_pairs_df = pd.DataFrame(columns=['base','quote'])\n",
    "        for key, value in enumerate(kraken_pairs):\n",
    "            kraken_pairs_df.loc[key] = [value['baseTradingSymbol'],value['quoteTradingSymbol']]    \n",
    "        return kraken_pairs_df\n",
    "\n",
    "    # Create Dataframes\n",
    "    kraken_pairs_df = calc_kraken_pairs_dataframe() #call function to create pairs dataframe\n",
    "    kraken_usd_pairs_df = kraken_pairs_df[(kraken_pairs_df['quote']=='USD')] \n",
    "    kraken_xbt_pairs_df = kraken_pairs_df[(kraken_pairs_df['quote']=='XBT')]   \n",
    "    \n",
    "    #Fetch Kraken Prices\n",
    "    def get_prices(kraken_pairs_df, interval, starttime):\n",
    "        master_prices_df = pd.DataFrame() \n",
    "        for index, row in kraken_pairs_df.iterrows():\n",
    "            candles = shrimpy_client.get_candles('kraken', row['base'], row['quote'], interval, starttime)\n",
    "            time = []\n",
    "            prices = []\n",
    "            for key, value in enumerate(candles):\n",
    "                time.append(value['time'])\n",
    "                prices.append(value['close'])\n",
    "            prices_df = pd.DataFrame(list(zip(time, prices)), columns = ['time', row['base']+ \"_\" + row['quote']])\n",
    "            prices_df['time'] = pd.to_datetime(prices_df['time'])\n",
    "            if master_prices_df.empty:\n",
    "                master_prices_df = prices_df\n",
    "            else:\n",
    "                master_prices_df = pd.merge(master_prices_df, prices_df, left_on='time', right_on = 'time', how = 'left')        \n",
    "        return master_prices_df\n",
    "    \n",
    "    # Create 1hr Price Dataframes for CryptoAssets quoted in XBT\n",
    "    df_xbt_prices_1h = get_prices(kraken_xbt_pairs_df, '1h', '2016-01-01') # Call get_prices Function\n",
    "    df_prices_xbt_1h = df_xbt_prices_1h.set_index('time') # set index to time\n",
    "    df_prices_xbt_1h = df_prices_xbt_1h.apply(pd.to_numeric) # Convert Prices to Floats\n",
    "    df_prices_xbt_1h = df_prices_xbt_1h.fillna(method='backfill') # Backfill null values\n",
    "    df_prices_xbt_1h = df_prices_xbt_1h.drop(columns = ['IC_XBT', 'TR_XBT']) # Delete problem symbols\n",
    "\n",
    "    # Create 1hr Price Dataframes for Cryptoassets quoted in USD\n",
    "    #df_usd_prices_1h = get_prices(kraken_usd_pairs_df, '1h', '2016-01-01') # Call get_prices Function\n",
    "    #df_prices_usd_1h = df_usd_prices_1h.set_index('time') # set index to time\n",
    "    #df_prices_usd_1h = df_prices_usd_1h.apply(pd.to_numeric) # Convert Prices to Floats\n",
    "    #df_prices_usd_1h = df_prices_usd_1h.fillna(method='backfill') # Backfill null values\n",
    "    \n",
    "    price_velocity_1h = df_prices_xbt_1h.pct_change(1)\n",
    "    price_velocity_2h = df_prices_xbt_1h.pct_change(2)\n",
    "    price_velocity_3h = df_prices_xbt_1h.pct_change(3)\n",
    "    price_velocity_4h = df_prices_xbt_1h.pct_change(4)\n",
    "    price_velocity_24h = df_prices_xbt_1h.pct_change(24)\n",
    "    price_velocity_168h = df_prices_xbt_1h.pct_change(168)\n",
    "    price_acceleration_1h = price_velocity_1h.pct_change(1)\n",
    "    price_acceleration_2h = price_velocity_2h.pct_change(2)\n",
    "    price_acceleration_3h = price_velocity_3h.pct_change(3)\n",
    "    price_acceleration_4h = price_velocity_4h.pct_change(4)\n",
    "    price_acceleration_24h = price_velocity_24h.pct_change(24)\n",
    "    price_acceleration_168h = price_velocity_168h.pct_change(168)\n",
    "    ema_9h = price_velocity_1h.ewm(halflife=9).mean()\n",
    "    ema_21h = price_velocity_1h.ewm(halflife=21).mean()\n",
    "    ema_crossover_long = np.where(ema_9h > ema_21h, 1.0, 0.0)\n",
    "    ema_crossover_short = np.where(ema_9h < ema_21h, 1.0, 0.0)\n",
    "    \n",
    "    kraken_prices_1h = pd.DataFrame(price_velocity_1h.unstack())\n",
    "    kraken_prices_1h.rename(columns={0: 'price_velocity_1h'}, inplace = True)\n",
    "    kraken_prices_1h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_a_1h = pd.DataFrame(price_acceleration_1h.unstack())\n",
    "    kraken_a_1h.rename(columns={0: 'price_acceleration_1h'}, inplace = True)\n",
    "    kraken_a_1h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_prices_2h = pd.DataFrame(price_velocity_2h.unstack())\n",
    "    kraken_prices_2h.rename(columns={0: 'price_velocity_2h'}, inplace = True)\n",
    "    kraken_prices_2h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_a_2h = pd.DataFrame(price_acceleration_2h.unstack())\n",
    "    kraken_a_2h.rename(columns={0: 'price_acceleration_2h'}, inplace = True)\n",
    "    kraken_a_2h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_prices_3h = pd.DataFrame(price_velocity_3h.unstack())\n",
    "    kraken_prices_3h.rename(columns={0: 'price_velocity_3h'}, inplace = True)\n",
    "    kraken_prices_3h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_a_3h = pd.DataFrame(price_acceleration_3h.unstack())\n",
    "    kraken_a_3h.rename(columns={0: 'price_acceleration_3h'}, inplace = True)\n",
    "    kraken_a_3h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_prices_4h = pd.DataFrame(price_velocity_4h.unstack())\n",
    "    kraken_prices_4h.rename(columns={0: 'price_velocity_4h'}, inplace = True)\n",
    "    kraken_prices_4h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_a_4h = pd.DataFrame(price_acceleration_4h.unstack())\n",
    "    kraken_a_4h.rename(columns={0: 'price_acceleration_4h'}, inplace = True)\n",
    "    kraken_a_4h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_prices_24h = pd.DataFrame(price_velocity_24h.unstack())\n",
    "    kraken_prices_24h.rename(columns={0: 'price_velocity_24h'}, inplace = True)\n",
    "    kraken_prices_24h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_a_24h = pd.DataFrame(price_acceleration_24h.unstack())\n",
    "    kraken_a_24h.rename(columns={0: 'price_acceleration_24h'}, inplace = True)\n",
    "    kraken_a_24h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_prices_168h = pd.DataFrame(price_velocity_168h.unstack())\n",
    "    kraken_prices_168h.rename(columns={0: 'price_velocity_168h'}, inplace = True)\n",
    "    kraken_prices_168h.reset_index(inplace=True)\n",
    "\n",
    "    kraken_a_168h = pd.DataFrame(price_acceleration_168h.unstack())\n",
    "    kraken_a_168h.rename(columns={0: 'price_acceleration_168h'}, inplace = True)\n",
    "    kraken_a_168h.reset_index(inplace=True)\n",
    "\n",
    "    ema_9 = pd.DataFrame(ema_9h.unstack())\n",
    "    ema_9.rename(columns={0: 'ema_9'}, inplace=True)\n",
    "    ema_9.reset_index(inplace=True)\n",
    "\n",
    "    ema_21 = pd.DataFrame(ema_21h.unstack())\n",
    "    ema_21.rename(columns={0: 'ema_21'}, inplace=True)\n",
    "    ema_21.reset_index(inplace=True)\n",
    "    \n",
    "    \n",
    "    factor_df_1 = pd.merge(kraken_prices_1h,kraken_a_1h, on=['level_0','time'], how='left')\n",
    "    factor_df_2 = pd.merge(factor_df_1,kraken_prices_2h, on=['level_0','time'], how='left')\n",
    "    factor_df_3 = pd.merge(factor_df_2,kraken_a_2h, on=['level_0','time'], how='left')\n",
    "    factor_df_4 = pd.merge(factor_df_3,kraken_prices_3h, on=['level_0','time'], how='left')\n",
    "    factor_df_5 = pd.merge(factor_df_4,kraken_a_3h, on=['level_0','time'], how='left')\n",
    "    factor_df_6 = pd.merge(factor_df_5,kraken_prices_4h, on=['level_0','time'], how='left')\n",
    "    factor_df_7 = pd.merge(factor_df_6,kraken_a_4h, on=['level_0','time'], how='left')\n",
    "    factor_df_8 = pd.merge(factor_df_7,kraken_prices_24h, on=['level_0','time'], how='left')\n",
    "    factor_df_9 = pd.merge(factor_df_8,kraken_a_24h, on=['level_0','time'], how='left')\n",
    "    factor_df_10 = pd.merge(factor_df_9,kraken_prices_168h, on=['level_0','time'], how='left')\n",
    "    factor_df_11 = pd.merge(factor_df_10,kraken_a_168h, on=['level_0','time'], how='left')\n",
    "    factor_df_12 = pd.merge(factor_df_11,ema_9, on=['level_0','time'], how='left')\n",
    "    factor_df_13 = pd.merge(factor_df_12,ema_21, on=['level_0','time'], how='left')\n",
    "    \n",
    "    return factor_df_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_df = calc_alt_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>time</th>\n",
       "      <th>price_velocity_1h</th>\n",
       "      <th>price_acceleration_1h</th>\n",
       "      <th>price_velocity_2h</th>\n",
       "      <th>price_acceleration_2h</th>\n",
       "      <th>price_velocity_3h</th>\n",
       "      <th>price_acceleration_3h</th>\n",
       "      <th>price_velocity_4h</th>\n",
       "      <th>price_acceleration_4h</th>\n",
       "      <th>price_velocity_24h</th>\n",
       "      <th>price_acceleration_24h</th>\n",
       "      <th>price_velocity_168h</th>\n",
       "      <th>price_acceleration_168h</th>\n",
       "      <th>ema_9</th>\n",
       "      <th>ema_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADA_XBT</td>\n",
       "      <td>2020-07-03 05:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADA_XBT</td>\n",
       "      <td>2020-07-03 06:00:00+00:00</td>\n",
       "      <td>-0.000976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000976</td>\n",
       "      <td>-0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA_XBT</td>\n",
       "      <td>2020-07-03 07:00:00+00:00</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-11.009766</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA_XBT</td>\n",
       "      <td>2020-07-03 08:00:00+00:00</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>-0.603868</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.004272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADA_XBT</td>\n",
       "      <td>2020-07-03 09:00:00+00:00</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>-2.245183</td>\n",
       "      <td>-0.000967</td>\n",
       "      <td>-1.110144</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.001886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0                      time  price_velocity_1h  \\\n",
       "0  ADA_XBT 2020-07-03 05:00:00+00:00                NaN   \n",
       "1  ADA_XBT 2020-07-03 06:00:00+00:00          -0.000976   \n",
       "2  ADA_XBT 2020-07-03 07:00:00+00:00           0.009766   \n",
       "3  ADA_XBT 2020-07-03 08:00:00+00:00           0.003868   \n",
       "4  ADA_XBT 2020-07-03 09:00:00+00:00          -0.004817   \n",
       "\n",
       "   price_acceleration_1h  price_velocity_2h  price_acceleration_2h  \\\n",
       "0                    NaN                NaN                    NaN   \n",
       "1                    NaN                NaN                    NaN   \n",
       "2             -11.009766           0.008780                    NaN   \n",
       "3              -0.603868           0.013672                    NaN   \n",
       "4              -2.245183          -0.000967              -1.110144   \n",
       "\n",
       "   price_velocity_3h  price_acceleration_3h  price_velocity_4h  \\\n",
       "0                NaN                    NaN                NaN   \n",
       "1                NaN                    NaN                NaN   \n",
       "2                NaN                    NaN                NaN   \n",
       "3           0.012683                    NaN                NaN   \n",
       "4           0.008789                    NaN           0.007805   \n",
       "\n",
       "   price_acceleration_4h  price_velocity_24h  price_acceleration_24h  \\\n",
       "0                    NaN                 NaN                     NaN   \n",
       "1                    NaN                 NaN                     NaN   \n",
       "2                    NaN                 NaN                     NaN   \n",
       "3                    NaN                 NaN                     NaN   \n",
       "4                    NaN                 NaN                     NaN   \n",
       "\n",
       "   price_velocity_168h  price_acceleration_168h     ema_9    ema_21  \n",
       "0                  NaN                      NaN       NaN       NaN  \n",
       "1                  NaN                      NaN -0.000976 -0.000976  \n",
       "2                  NaN                      NaN  0.004602  0.004484  \n",
       "3                  NaN                      NaN  0.004338  0.004272  \n",
       "4                  NaN                      NaN  0.001779  0.001886  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need to drop infinite values and would like to add the partial differences, but maybe that just goes in areas to improve\n",
    "[drop infinite values](https://stackoverflow.com/questions/17477979/dropping-infinite-values-from-dataframes-in-pandas)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_0                        0\n",
       "time                           0\n",
       "price_velocity_1h             33\n",
       "price_acceleration_1h       3451\n",
       "price_velocity_2h             66\n",
       "price_acceleration_2h       2653\n",
       "price_velocity_3h             99\n",
       "price_acceleration_3h       2495\n",
       "price_velocity_4h            132\n",
       "price_acceleration_4h       2469\n",
       "price_velocity_24h           792\n",
       "price_acceleration_24h      3396\n",
       "price_velocity_168h         5544\n",
       "price_acceleration_168h    11532\n",
       "ema_9                         33\n",
       "ema_21                        33\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "    1. Use window_data function to generate the X and y values for the model (rolling 40 hour window to predict 41 and 44 hr price change)\n",
    "    2. Split the data into 7-% training and 30% testing\n",
    "    3. Apply the MinMaxScaler to the X and y values\n",
    "    4. Reshape the X_train and X_test data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_algorithm(): \n",
    "    # Set environment variables from the .env file\n",
    "    env_path = Path(\"/Users/GrantDePalma\")/'.env'\n",
    "    load_dotenv(env_path)\n",
    "    # Extract API Keys from environment variables\n",
    "    shrimpy_public_key = os.getenv(\"SHRIMPY_PUBLIC_KEY\")\n",
    "    shrimpy_private_key = os.getenv(\"SHRIMPY_PRIVATE_KEY\")\n",
    "    KRAKEN_API_KEY = os.getenv(\"KRAKEN_API_KEY\")\n",
    "    KRAKEN_PRIVATE_KEY = os.getenv(\"KRAKEN_PRIVATE_KEY\")\n",
    "\n",
    "    # Create User ID\n",
    "    users = client.list_users()\n",
    "    user_id = users[0]['id']\n",
    "\n",
    "    # Exchange Data\n",
    "    exchange_name = 'kraken'\n",
    "    exchange_public_key = KRAKEN_API_KEY\n",
    "    exchange_secret_key = KRAKEN_SECRET_KEY\n",
    "    # Link kraken exchange \n",
    "    link_account_response = client.link_account(user_id,\n",
    "                                               exchange_name,\n",
    "                                               exchange_public_key,\n",
    "                                               exchange_secret_key)\n",
    "    account_id = link_account_response['id']\n",
    "\n",
    "    #balances\n",
    "    balance = client.get_balance(user_id, account_id)\n",
    "    total_balance_history = client.get_total_balance_history(user_id, account_id)\n",
    "    \n",
    "    btc_wts = []\n",
    "    usd_wts = [0,.25,.5]\n",
    "    alt_wts = [0,.25,.5]\n",
    "    \n",
    "    very_bullish = # if btc price prediction is greater than \n",
    "    bullish =  # if btc\n",
    "    nuetral =  # if btc is predicting 0 returns\n",
    "    bearish =  # if btc is predicting negative returns\n",
    "    very_bearish = # if btci predicting very negative returns\n",
    "    \n",
    "    if very_bullish: #>5%\n",
    "        btc_wt = .5\n",
    "        usd_wt = 0\n",
    "        alt_wt = .5\n",
    "        \n",
    "    elif bullish: #0->5%\n",
    "        btc_wt = .4\n",
    "        usd_wt = .2\n",
    "        alt_wt = .4\n",
    "        \n",
    "    elif nuetral: #-1% --> 0%\n",
    "        btc_wt = .2\n",
    "        usd_wt = .6\n",
    "        alt_wt = .2\n",
    "        \n",
    "    elif bearish: # -5% ---> -1%\n",
    "        btc_wt = .1\n",
    "        usd_wt = .8\n",
    "        alt_wt = .1\n",
    "    \n",
    "    elif very_bearish:  # < -5%\n",
    "        btc_wt = 0 \n",
    "        usd_wt = 1\n",
    "        alt_wt = 0\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Asset Management\n",
    "    def strategy():\n",
    "        shrimpy_client.set_rebalance_period(user_id, account_id, 1) # set rebalance every 1 hour\n",
    "        shrimpy_client.set_strategy(user_id, account_id, {\n",
    "            \"isDynamic\":false,\n",
    "            \"allocations\": [\n",
    "                {\"symbol\": \"XBT\", \"percent\":btc_wt},\n",
    "                {\"symbol\": \"USD\", \"percent\":usd_wt},\n",
    "                {\"symbol\": alt_1, \"percent\": alt_wt_1},\n",
    "                {\"symbol\": alt_2, \"percent\": alt_wt_2},\n",
    "                {\"symbol\": alt_3, \"percent\": alt_wt_3},\n",
    "                {\"symbol\": alt_4, \"percent\": alt_wt_4},\n",
    "                {\"symbol\": alt_5, \"percent\": alt_wt_5},\n",
    "                {\"symbol\": alt_6, \"percent\": alt_wt_6},\n",
    "                {\"symbol\": alt_7, \"percent\": alt_wt_7},\n",
    "                {\"symbol\": alt_8, \"percent\": alt_wt_8},\n",
    "                {\"symbol\": alt_9, \"percent\": alt_wt_9},\n",
    "                {\"symbol\": alt_10, \"percent\": alt_wt_10},\n",
    "            ]\n",
    "        })\n",
    "        shrimpy_client.rebalance(user_id, account_id) # Rebalance, put in rebal\n",
    "    \n",
    "    \n",
    "    # sell every asset besides the consolidation asset\n",
    "    for asset in holdings:\n",
    "        asset_symbol = asset['symbol']\n",
    "        asset_amount = asset['nativeValue']\n",
    "        if asset_symbol != consolidation_symbol:\n",
    "            print('Selling ' + str(asset_amount) + ' of ' + asset_symbol)\n",
    "            create_trade_response = client.create_trade(\n",
    "                user_id,\n",
    "                account_id,\n",
    "                asset_symbol,\n",
    "                consolidation_symbol,\n",
    "                asset_amount\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "# Live data feed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
